<!doctype html>




<html class="theme-next pisces" lang="en">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>



<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />



  <meta name="google-site-verification" content="NjP189VhFlmwjqBiqOeGB6U7MJXKXL9W0mO_oqF9rKY" />










  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />




  
  
  
  

  
    
    
  

  

  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.0" rel="stylesheet" type="text/css" />


  <meta name="keywords" content="tensorflow,deeplearning,ithome鐵人," />








  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.1.0" />






<meta name="description" content="今日目標
了解 Sparse Autoencoder
了解 KL divergence &amp;amp; L2 loss
實作 Sparse Autoencoder">
<meta property="og:type" content="article">
<meta property="og:title" content="Tensorflow Day17 Sparse Autoencoder">
<meta property="og:url" content="https://blog.c1mone.com.tw/2017/01/01/tensorflow-note-day-17/index.html">
<meta property="og:site_name" content="94 讀書筆記咩">
<meta property="og:description" content="今日目標
了解 Sparse Autoencoder
了解 KL divergence &amp;amp; L2 loss
實作 Sparse Autoencoder">
<meta property="og:image" content="http://imgur.com/2JFGz0N.jpg">
<meta property="og:image" content="http://imgur.com/jjDYAM0.jpg">
<meta property="og:image" content="http://imgur.com/3FB8PL5.jpg">
<meta property="og:updated_time" content="2017-06-24T13:45:37.917Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Tensorflow Day17 Sparse Autoencoder">
<meta name="twitter:description" content="今日目標
了解 Sparse Autoencoder
了解 KL divergence &amp;amp; L2 loss
實作 Sparse Autoencoder">
<meta name="twitter:image" content="http://imgur.com/2JFGz0N.jpg">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    sidebar: {"position":"left","display":"post"},
    fancybox: true,
    motion: false,
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="https://blog.c1mone.com.tw/2017/01/01/tensorflow-note-day-17/"/>





  <title> Tensorflow Day17 Sparse Autoencoder | 94 讀書筆記咩 </title>
</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="en">

  


<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
            (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
          m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
  ga('create', 'UA-101607299-1', 'auto');
  ga('send', 'pageview');
</script>









  
  
    
  

  <div class="container one-collumn sidebar-position-left page-post-detail ">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-meta ">
  

  <div class="custom-logo-site-title">
    <a href="/"  class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <span class="site-title">94 讀書筆記咩</span>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>
  <p class="site-subtitle">Be relentlessly resourceful．</p>
</div>

<div class="site-nav-toggle">
  <button>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
  </button>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            About
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            Archives
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            Categories
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            Tags
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
  <link itemprop="mainEntityOfPage" href="https://blog.c1mone.com.tw/2017/01/01/tensorflow-note-day-17/">

  <span style="display:none" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <meta itemprop="name" content="c1mone">
    <meta itemprop="description" content="">
    <meta itemprop="image" content="https://avatars2.githubusercontent.com/u/4519639?v=3&s=400.jpg">
  </span>

  <span style="display:none" itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
    <meta itemprop="name" content="94 讀書筆記咩">
    <span style="display:none" itemprop="logo" itemscope itemtype="http://schema.org/ImageObject">
      <img style="display:none;" itemprop="url image" alt="94 讀書筆記咩" src="">
    </span>
  </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                Tensorflow Day17 Sparse Autoencoder
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2017-01-01T06:01:06+00:00">
                2017-01-01
              </time>
            

            

            
          </span>

          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <a href="/2017/01/01/tensorflow-note-day-17/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2017/01/01/tensorflow-note-day-17/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          

          
          

          

          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        <h2 id="今日目標"><a href="#今日目標" class="headerlink" title="今日目標"></a>今日目標</h2><ul>
<li>了解 Sparse Autoencoder</li>
<li>了解 KL divergence &amp; L2 loss</li>
<li>實作 Sparse Autoencoder</li>
</ul>
<a id="more"></a>
<p><a href="https://github.com/c1mone/Tensorflow-101/blob/master/notebooks/6_Sparse_Autoencoder.ipynb" target="_blank" rel="external">Github Ipython Notebook 好讀完整版</a></p>
<p>當在訓練一個普通的 <code>autoenoder</code> 時，如果嘗試丟入一些輸入，會看到中間許多的神經元 (hidden unit) 大部分都會有所反應 (activate)．反應的意思是這個神經元的輸出不會等於零，也不會很接近零，而是大於零許多．白話的意思就是神經元說：「咦！這個輸入我認識噢～」</p>
<p>然而我們是不想要看到這樣的情形的！我們想要看到的情形是每個神經元只對一些些訓練輸入有反應．例如手寫數字 0-9，那神經元 A 只對數字 5 有反應，神經元 B 只對 7 有反應 … 等．為什麼要這樣的結果呢？在 <a href="https://www.quora.com/Why-are-sparse-autoencoders-sparse" target="_blank" rel="external">Quora</a> 上面有一個解說是這樣的</p>
<blockquote>
<p>如果一個人可以做 A, B, C … 許多的工作，那他就不太可能是 A 工作的專家，或是 B 工作的專家．<br>如果一個神經元對於每個不同的訓練都會有反應，那有它沒它好像沒有什麼差別</p>
</blockquote>
<p>所以接下來要做的事情就是加上稀疏的限制條件 (sparse constraint)，來訓練出 <code>Sparse Autoencoder</code>．而要在哪裡加上這個限制呢？就是要在 loss 函數中做手腳．在這裡我們會加上兩個項，分別是：</p>
<ul>
<li>Sparsity Regularization</li>
<li>L2 Regularization</li>
</ul>
<h2 id="Sparsity-Regularization"><a href="#Sparsity-Regularization" class="headerlink" title="Sparsity Regularization"></a>Sparsity Regularization</h2><p>這一項我們想要做的事就是讓 autoencoder 中每個神經元的輸出變小，而實際上的做法則是如下<br><strong>先設定一個值，然後讓平均神經元輸出值 (average output activation vlue) 越接近它越好，如果偏離這個值，cost 函數就會變大，達到懲罰的效果</strong></p>
<p>$$<br>\hat{\rho<em>{i}} = \frac{1}{n} \sum</em>{j = 1}^{n} h(w<em>{i}^{T}  x</em>{j} + b<em>{i})<br>\<br>\hat{\rho</em>{i}} : \text{ average output activation value of a neuron i}<br>\<br>n: \text{ total number of training examples}<br>\<br>x<em>{j}: \text{jth training example}<br>\<br>w</em>{i}^{T}: \text{ith row of  the weight matrix W}<br>\<br>b_{i}: \text{ith entropy of the bias vector}<br>\<br>$$</p>
<h3 id="Kullback-Leibler-divergence-relative-entropy"><a href="#Kullback-Leibler-divergence-relative-entropy" class="headerlink" title="Kullback-Leibler divergence (relative entropy)"></a>Kullback-Leibler divergence (relative entropy)</h3><p>$$<br>\Omega<em>{sparsity} = \sum</em>{i=1}^{D}\rho\log(\frac{\rho}{\hat{\rho<em>{i}}})+(1-\rho)\log(\frac{1-\rho}{1-\hat{\rho</em>{i}}})<br>\<br>\hat{\rho_{i}} : \text{ average output activation value of a neuron i}<br>$$</p>
<p><code>Kullback-Leibler divergence</code> 是用來計算兩個機率分佈接近的程度，如果兩個一樣的話就為 0．我們可以看以下的例子，設定值 rho_hat 為 0.2，而 rho 等於 0.2 的時候 kl_div = 0，rho 等於其他值時 kl_div 大於 0．</p>
<p>而在實例上，就讓 rho 以 average output activation 取代．</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line">%matplotlib inline</div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</div><div class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</div><div class="line"><span class="keyword">from</span> tensorflow.examples.tutorials.mnist <span class="keyword">import</span> input_data</div><div class="line">mnist = input_data.read_data_sets(<span class="string">"MNIST_data/"</span>, one_hot = <span class="keyword">True</span>)</div><div class="line"></div><div class="line">rho_hat = np.linspace(<span class="number">0</span> + <span class="number">1e-2</span>, <span class="number">1</span> - <span class="number">1e-2</span>, <span class="number">100</span>)</div><div class="line">rho = <span class="number">0.2</span></div><div class="line">kl_div = rho * np.log(rho/rho_hat) + (<span class="number">1</span> - rho) * np.log((<span class="number">1</span> - rho) / (<span class="number">1</span> - rho_hat))</div><div class="line">plt.plot(rho_hat, kl_div)</div><div class="line">plt.xlabel(<span class="string">"rho_hat"</span>)</div><div class="line">plt.ylabel(<span class="string">"kl_div"</span>)</div></pre></td></tr></table></figure>
<p><img src="http://imgur.com/2JFGz0N.jpg" alt=""></p>
<h3 id="L2-Regularization"><a href="#L2-Regularization" class="headerlink" title="L2 Regularization"></a>L2 Regularization</h3><p>經過了 Sparsity Regularization 這一項，理想上神經元輸出會接近我們所設定的值．而這裡想要達到的目標就是讓 weight 盡量的變小，讓整個模型變得比較簡單，而不是 weight 變大，使得 bias 要變得很大來修正．<br>$$<br>\Omega<em>{weights} = \frac{1}{2}\sum</em>{l}^{L}\sum<em>{j}^{n}\sum</em>{i}^{k}(w_{ji}^{(l)})^{2}<br>\<br>L : \text{number of the hidden layers}<br>\<br>n : \text{number of observations}<br>\<br>k : \text{number of variables in training data}<br>$$</p>
<h3 id="cost-函數"><a href="#cost-函數" class="headerlink" title="cost 函數"></a>cost 函數</h3><p>cost 函數就是把這幾項全部加起來，來 minimize 它．</p>
<p>$$<br>E = \Omega<em>{mse} + \beta * \Omega</em>{sparsity} + \lambda * \Omega_{weights}<br>$$</p>
<p>在 tensorflow 裡面有現成的函數 <code>tf.nn.l2_loss</code> 可以使用，把單一層的 l2_loss 計算出來，舉個例子，如果有兩層隱層權重 <code>w1</code>, <code>w2</code>，則要把兩個加總  <code>tf.nn.l2_loss(w1) + tf.nn.l2_loss(w2)</code></p>
<h2 id="實作"><a href="#實作" class="headerlink" title="實作"></a>實作</h2><p>我們會先建立一個一般的 autoencoder，之後再建立一個 sparse autoencoder，並比較它輸出的影像以及 average activation output value．</p>
<h3 id="Normal-Autoencoder"><a href="#Normal-Autoencoder" class="headerlink" title="Normal Autoencoder"></a>Normal Autoencoder</h3><p>建立 784 -&gt; 300 -&gt; 30 -&gt; 300 -&gt; 784 Autoencoder，</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">build_sae</span><span class="params">()</span>:</span></div><div class="line">    W_e_1 = weight_variable([<span class="number">784</span>, <span class="number">300</span>], <span class="string">"w_e_1"</span>)</div><div class="line">    b_e_1 = bias_variable([<span class="number">300</span>], <span class="string">"b_e_1"</span>)</div><div class="line">    h_e_1 = tf.nn.sigmoid(tf.add(tf.matmul(x, W_e_1), b_e_1))</div><div class="line"></div><div class="line">    W_e_2 = weight_variable([<span class="number">300</span>, <span class="number">30</span>], <span class="string">"w_e_2"</span>)</div><div class="line">    b_e_2 = bias_variable([<span class="number">30</span>], <span class="string">"b_e_2"</span>)</div><div class="line">    h_e_2 = tf.nn.sigmoid(tf.add(tf.matmul(h_e_1, W_e_2), b_e_2))</div><div class="line"></div><div class="line">    W_d_1 = weight_variable([<span class="number">30</span>, <span class="number">300</span>], <span class="string">"w_d_1"</span>)</div><div class="line">    b_d_1 = bias_variable([<span class="number">300</span>], <span class="string">"b_d_1"</span>)</div><div class="line">    h_d_1 = tf.nn.sigmoid(tf.add(tf.matmul(h_e_2, W_d_1), b_d_1))</div><div class="line"></div><div class="line">    W_d_2 = weight_variable([<span class="number">300</span>, <span class="number">784</span>], <span class="string">"w_d_2"</span>)</div><div class="line">    b_d_2 = bias_variable([<span class="number">784</span>], <span class="string">"b_d_2"</span>)</div><div class="line">    h_d_2 = tf.nn.sigmoid(tf.add(tf.matmul(h_d_1, W_d_2), b_d_2))</div><div class="line">    </div><div class="line">    <span class="keyword">return</span> [h_e_1, h_e_2], [W_e_1, W_e_2, W_d_1, W_d_2], h_d_2</div></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line">tf.reset_default_graph()</div><div class="line">sess = tf.InteractiveSession()</div><div class="line">x = tf.placeholder(tf.float32, shape = [<span class="keyword">None</span>, <span class="number">784</span>])</div><div class="line">h, w, x_reconstruct = build_sae()</div><div class="line"></div><div class="line">loss = tf.reduce_mean(tf.pow(x_reconstruct - x, <span class="number">2</span>))</div><div class="line">optimizer = tf.train.AdamOptimizer(<span class="number">0.01</span>).minimize(loss)</div><div class="line">init_op = tf.global_variables_initializer()</div><div class="line"></div><div class="line">sess.run(init_op)</div><div class="line"></div><div class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">20000</span>):</div><div class="line">    batch = mnist.train.next_batch(<span class="number">60</span>)</div><div class="line">    <span class="keyword">if</span> i%<span class="number">100</span> == <span class="number">0</span>:</div><div class="line">        print(<span class="string">"step %d, loss %g"</span>%(i, loss.eval(feed_dict=&#123;x:batch[<span class="number">0</span>]&#125;)))</div><div class="line">    optimizer.run(feed_dict=&#123;x: batch[<span class="number">0</span>]&#125;)</div><div class="line">    </div><div class="line">print(<span class="string">"final loss %g"</span> % loss.eval(feed_dict=&#123;x: mnist.test.images&#125;))</div></pre></td></tr></table></figure>
<pre><code>step 0, loss 0.259796
step 100, loss 0.0712686
step 200, loss 0.056199
step 300, loss 0.0586076
step 400, loss 0.0488305
step 500, loss 0.0377571
step 600, loss 0.0372789
step 700, loss 0.0319157
step 800, loss 0.0314859
step 900, loss 0.0278508
step 1000, loss 0.0256422
step 1100, loss 0.0272346
step 1200, loss 0.0241254
step 1300, loss 0.023016
step 1400, loss 0.0212343
step 1500, loss 0.0179811
step 2000, loss 0.0155893
step 3000, loss 0.0145139
step 4000, loss 0.0117702
step 5000, loss 0.0119975
step 6000, loss 0.0106937
step 7000, loss 0.0113036
step 8000, loss 0.00997475
step 9000, loss 0.0116126
step 10000, loss 0.0104301
step 11000, loss 0.00969182
step 12000, loss 0.00969755
step 13000, loss 0.0104931
step 14000, loss 0.00950653
step 15000, loss 0.00963279
step 16000, loss 0.0098329
step 17000, loss 0.00817896
step 18000, loss 0.00903721
step 19000, loss 0.00828982
final loss 0.00885361
</code></pre><h4 id="average-output-activation-value"><a href="#average-output-activation-value" class="headerlink" title="average output activation value"></a>average output activation value</h4><p>印出 encoder 中第一層以及第二層的 <code>average output activation value</code></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">for</span> h_i <span class="keyword">in</span> h:</div><div class="line">    print(<span class="string">"average output activation value %g"</span> % tf.reduce_mean(h_i).eval(feed_dict=&#123;x: mnist.test.images&#125;))</div></pre></td></tr></table></figure>
<pre><code>average output activation value 0.191295
average output activation value 0.378384
</code></pre><p><img src="http://imgur.com/jjDYAM0.jpg" alt=""></p>
<h2 id="Sparse-Autoencoder"><a href="#Sparse-Autoencoder" class="headerlink" title="Sparse Autoencoder"></a>Sparse Autoencoder</h2><h3 id="KL-divergence-function"><a href="#KL-divergence-function" class="headerlink" title="KL divergence function"></a>KL divergence function</h3><p>依照公式建立 kl_div 函數</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">kl_div</span><span class="params">(rho, rho_hat)</span>:</span></div><div class="line">    invrho = tf.sub(tf.constant(<span class="number">1.</span>), rho)</div><div class="line">    invrhohat = tf.sub(tf.constant(<span class="number">1.</span>), rho_hat)</div><div class="line">    logrho = tf.add(logfunc(rho,rho_hat), logfunc(invrho, invrhohat))</div><div class="line">    <span class="keyword">return</span> logrho</div><div class="line">    </div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">logfunc</span><span class="params">(x, x2)</span>:</span></div><div class="line">    <span class="keyword">return</span> tf.mul( x, tf.log(tf.div(x,x2)))</div></pre></td></tr></table></figure>
<h3 id="loss-function"><a href="#loss-function" class="headerlink" title="loss function"></a>loss function</h3><p>把三個 loss 全部加起來，並乘以對應的係數</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div></pre></td><td class="code"><pre><div class="line">tf.reset_default_graph()</div><div class="line">sess = tf.InteractiveSession()</div><div class="line">x = tf.placeholder(tf.float32, shape = [<span class="keyword">None</span>, <span class="number">784</span>])</div><div class="line">h, w, x_reconstruct = build_sae()</div><div class="line"></div><div class="line">alpha = <span class="number">5e-6</span></div><div class="line">beta = <span class="number">7.5e-5</span></div><div class="line">kl_div_loss = reduce(<span class="keyword">lambda</span> x, y: x + y, map(<span class="keyword">lambda</span> x: tf.reduce_sum(kl_div(<span class="number">0.02</span>, tf.reduce_mean(x,<span class="number">0</span>))), h))</div><div class="line"><span class="comment">#kl_div_loss = tf.reduce_sum(kl_div(0.02, tf.reduce_mean(h[0],0)))</span></div><div class="line">l2_loss = reduce(<span class="keyword">lambda</span> x, y: x + y, map(<span class="keyword">lambda</span> x: tf.nn.l2_loss(x), w))</div><div class="line"></div><div class="line">loss = tf.reduce_mean(tf.pow(x_reconstruct - x, <span class="number">2</span>)) + alpha * l2_loss + beta * kl_div_loss</div><div class="line">optimizer = tf.train.AdamOptimizer(<span class="number">0.01</span>).minimize(loss)</div><div class="line">init_op = tf.global_variables_initializer()</div><div class="line"></div><div class="line">sess.run(init_op)</div><div class="line"></div><div class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">20000</span>):</div><div class="line">    batch = mnist.train.next_batch(<span class="number">60</span>)</div><div class="line">    <span class="keyword">if</span> i%<span class="number">100</span> == <span class="number">0</span>:</div><div class="line">        print(<span class="string">"step %d, loss %g"</span>%(i, loss.eval(feed_dict=&#123;x:batch[<span class="number">0</span>]&#125;)))</div><div class="line">    optimizer.run(feed_dict=&#123;x: batch[<span class="number">0</span>]&#125;)</div><div class="line">    </div><div class="line">print(<span class="string">"final loss %g"</span> % loss.eval(feed_dict=&#123;x: mnist.test.images&#125;))</div></pre></td></tr></table></figure>
<pre><code>step 0, loss 0.283789
step 100, loss 0.0673799
step 200, loss 0.061653
step 300, loss 0.0575306
step 400, loss 0.0549822
step 500, loss 0.0485821
step 600, loss 0.0470816
step 700, loss 0.0441757
step 800, loss 0.042368
step 900, loss 0.0441069
step 1000, loss 0.0419031
step 1100, loss 0.0435174
step 1200, loss 0.0414619
step 1300, loss 0.0423286
step 1400, loss 0.0394959
step 1500, loss 0.0423292
step 2000, loss 0.0399037
step 3000, loss 0.0394368
step 4000, loss 0.0379597
step 5000, loss 0.035319
step 6000, loss 0.0351442
step 7000, loss 0.0376415
step 8000, loss 0.0366516
step 9000, loss 0.0382368
step 10000, loss 0.0357169
step 11000, loss 0.0366914
step 12000, loss 0.0382858
step 13000, loss 0.0349964
step 14000, loss 0.0370025
step 15000, loss 0.036228
step 16000, loss 0.0367592
step 17000, loss 0.0356757
step 18000, loss 0.0369231
step 19000, loss 0.0345381
final loss 0.0355583
</code></pre><h4 id="average-output-activation-value-1"><a href="#average-output-activation-value-1" class="headerlink" title="average output activation value"></a>average output activation value</h4><p>印出 encoder 中第一層以及第二層的 <code>average output activation value</code></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">for</span> h_i <span class="keyword">in</span> h:</div><div class="line">    print(<span class="string">"average output activation value %g"</span> % tf.reduce_mean(h_i).eval(feed_dict=&#123;x: mnist.test.images&#125;))</div></pre></td></tr></table></figure>
<pre><code>average output activation value 0.0529726
average output activation value 0.398633
</code></pre><p><img src="http://imgur.com/3FB8PL5.jpg" alt=""></p>
<p>圖片結果可以看到它和普通的 autoencoder 差不多，但是稍微糊了一點，而第一層的 average output activation value 從 0.19 降到了 0.05，第二層的值反而上升了一點點．這個部分的調整跟 hyperparameter 有很大的關係，如果我把 beta 調大，第一第二層的 average output activation value 會接近 0.02，但是輸出的圖像會變模糊．<code>beta = 7.5e-5</code> 是我試了幾次以後比較平衡兩者的結果．</p>
<h2 id="今日心得"><a href="#今日心得" class="headerlink" title="今日心得"></a>今日心得</h2><p>我們實現了 KL Divergence 以及 L2 loss，並把這兩個項加入了 loss，成為了 sparse autoencoder．最後的結果會看到 average output activation value 是有明顯下降的．</p>
<p>而整個過程需要花比較多時間的地方是在 hyperparameter 的調整，調太大或者調太小，都會沒辦法達到預期的效果．</p>
<h4 id="問題"><a href="#問題" class="headerlink" title="問題"></a>問題</h4><ul>
<li>如果改用 L1 loss 的結果?</li>
<li>有沒有更好的方法來決定 hyperparameter?</li>
<li>這裡的 activation function 都是 sigmoid，如果用 ReLU?</li>
</ul>
<h2 id="學習資源連結"><a href="#學習資源連結" class="headerlink" title="學習資源連結"></a>學習資源連結</h2><ul>
<li><a href="https://www.mathworks.com/help/nnet/ref/trainautoencoder.html#buythqy" target="_blank" rel="external">Matlab Autoencoder Doc</a></li>
<li><a href="https://blog.keras.io/building-autoencoders-in-keras.html" target="_blank" rel="external">Sparse Autoencoder in Keras</a></li>
</ul>

      
    </div>

    <div>
      
        

      
    </div>

    <div>
      
        

      
    </div>


    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/tensorflow/" rel="tag"># tensorflow</a>
          
            <a href="/tags/deeplearning/" rel="tag"># deeplearning</a>
          
            <a href="/tags/ithome鐵人/" rel="tag"># ithome鐵人</a>
          
        </div>
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2016/12/31/tensorflow-note-day-16/" rel="next" title="Tensorflow Day16 Autoencoder 實作">
                <i class="fa fa-chevron-left"></i> Tensorflow Day16 Autoencoder 實作
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2017/01/02/tensorflow-note-day-18/" rel="prev" title="Tensorflow Day18 Convolutional Autoencoder">
                Tensorflow Day18 Convolutional Autoencoder <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          
  <div class="comments" id="comments">
    
      <div id="disqus_thread">
        <noscript>
          Please enable JavaScript to view the
          <a href="//disqus.com/?ref_noscript">comments powered by Disqus.</a>
        </noscript>
      </div>
    
  </div>


        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap" >
            Table of Contents
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview">
            Overview
          </li>
        </ul>
      

      <section class="site-overview sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" itemprop="image"
               src="https://avatars2.githubusercontent.com/u/4519639?v=3&s=400.jpg"
               alt="c1mone" />
          <p class="site-author-name" itemprop="name">c1mone</p>
          <p class="site-description motion-element" itemprop="description">全棧數據工程師之路迢迢</p>
        </div>
        <nav class="site-state motion-element">
          <div class="site-state-item site-state-posts">
            <a href="/archives">
              <span class="site-state-item-count">30</span>
              <span class="site-state-item-name">posts</span>
            </a>
          </div>

          

          
            <div class="site-state-item site-state-tags">
              <a href="/tags">
                <span class="site-state-item-count">21</span>
                <span class="site-state-item-name">tags</span>
              </a>
            </div>
          

        </nav>

        

        <div class="links-of-author motion-element">
          
            
              <span class="links-of-author-item">
                <a href="http://github.com/c1mone" target="_blank" title="GitHub">
                  
                    <i class="fa fa-fw fa-github"></i>
                  
                  GitHub
                </a>
              </span>
            
          
        </div>

        
        

        
        

        


      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#今日目標"><span class="nav-number">1.</span> <span class="nav-text">今日目標</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Sparsity-Regularization"><span class="nav-number">2.</span> <span class="nav-text">Sparsity Regularization</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Kullback-Leibler-divergence-relative-entropy"><span class="nav-number">2.1.</span> <span class="nav-text">Kullback-Leibler divergence (relative entropy)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#L2-Regularization"><span class="nav-number">2.2.</span> <span class="nav-text">L2 Regularization</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#cost-函數"><span class="nav-number">2.3.</span> <span class="nav-text">cost 函數</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#實作"><span class="nav-number">3.</span> <span class="nav-text">實作</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Normal-Autoencoder"><span class="nav-number">3.1.</span> <span class="nav-text">Normal Autoencoder</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#average-output-activation-value"><span class="nav-number">3.1.1.</span> <span class="nav-text">average output activation value</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Sparse-Autoencoder"><span class="nav-number">4.</span> <span class="nav-text">Sparse Autoencoder</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#KL-divergence-function"><span class="nav-number">4.1.</span> <span class="nav-text">KL divergence function</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#loss-function"><span class="nav-number">4.2.</span> <span class="nav-text">loss function</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#average-output-activation-value-1"><span class="nav-number">4.2.1.</span> <span class="nav-text">average output activation value</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#今日心得"><span class="nav-number">5.</span> <span class="nav-text">今日心得</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#問題"><span class="nav-number">5.0.1.</span> <span class="nav-text">問題</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#學習資源連結"><span class="nav-number">6.</span> <span class="nav-text">學習資源連結</span></a></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright" >
  
  &copy;  2016 - 
  <span itemprop="copyrightYear">2017</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">c1mone</span>
</div>


<div class="powered-by">
  Powered by <a class="theme-link" href="https://hexo.io">Hexo</a>
</div>

<div class="theme-info">
  Theme -
  <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">
    NexT.Pisces
  </a>
</div>


        

        
      </div>
    </footer>

    <div class="back-to-top">
      <i class="fa fa-arrow-up"></i>
    </div>
  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  



  
  <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.0"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.0"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.0"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.0"></script>



  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.0"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.0"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.0"></script>



  



  

    <script type="text/javascript">
      var disqus_shortname = 'c1mone';
      var disqus_identifier = '2017/01/01/tensorflow-note-day-17/';

      var disqus_title = "Tensorflow Day17 Sparse Autoencoder";


      function run_disqus_script(disqus_script) {
        var dsq = document.createElement('script');
        dsq.type = 'text/javascript';
        dsq.async = true;
        dsq.src = '//' + disqus_shortname + '.disqus.com/' + disqus_script;
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
      }

      run_disqus_script('count.js');

      
        var disqus_config = function () {
            this.page.url = disqus_url;
            this.page.identifier = disqus_identifier;
            this.page.title = disqus_title;
        };
        run_disqus_script('embed.js');
      

    </script>
  





  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  

  


</body>
</html>
